{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching Tweets in Python \n",
    "\n",
    "## Copyleft 2020 Forrest Sheng Bao \n",
    "\n",
    "To get the code working, you need to get Twitter Developer Account. \n",
    "Then create a file `credentials.py` and put your Twitter API credentials in it, like this (the keys and secrete below do not work. Just examples): \n",
    "\n",
    "```\n",
    "consumer_key = \"xvz1evFS4wEEPTGEFPHBog\"\n",
    "consumer_secrete = \"L8qq9PZyRg6ieKGEKhZolGC0vJWLw8iEJ88DRdyOg\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"  # This one is optional. And you won't have it until your finish step 1. \n",
    "```\n",
    "\n",
    "Opinions expressed here do not reflect those of Iowa State University and Iowa NPR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import credentials # a user script containing keys, secrets, and tokens \n",
    "\n",
    "import json\n",
    "import base64\n",
    "import copy\n",
    "\n",
    "# use two (diversity!) libraries for making web requests\n",
    "import requests # for authentication\n",
    "import urllib   # for crawling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Get Twitter authentication (do it only once, unless you want a new token)\n",
    "\n",
    "We only need OAuth 2.0 Basic authentication because the script below only access public tweets. No need for OAuth 1.0 which accesses user-specific data. \n",
    "\n",
    "It will send a request to Twitter's server with your Twitter developer credentials (not your Twitter username and password). If correct, the server will return a Bearer access token. \n",
    "Include that token in the headers of all search queries in the future. \n",
    "\n",
    "If you have valid Bearer token, you can skip this step. \n",
    "The next step assumes Bearer token is saved in the `credential.py` file \n",
    "\n",
    "For more details: see \n",
    "https://developer.twitter.com/en/docs/basics/authentication/oauth-2-0/application-only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'access_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a7c94dccebc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbearer_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'access_token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'access_token'"
     ]
    }
   ],
   "source": [
    "consumer_key = credentials.consumer_key \n",
    "consumer_secrete = credentials.consumer_secrete\n",
    "\n",
    "bearer = \":\".join([consumer_key, consumer_secrete])\n",
    "bearer_base64 = str(base64.b64encode(bearer.encode('utf-8')))\n",
    "\n",
    "r = requests.post('https://api.twitter.com/oauth2/token',\n",
    "                   data={\"grant_type\":\"client_credentials\"},\n",
    "                   headers = {\"Authorization\": \"Basic \" + bearer_base64, \n",
    "                              \"Content-Type\": \"application/x-www-form-urlencoded;charset=UTF-8\"}\n",
    "                   )\n",
    "\n",
    "reply = json.loads(r.content)\n",
    "bearer_token = reply['access_token']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Search tweets using tags\n",
    "\n",
    "Let's do a very basic search: find all Tweets of the tag `#coronavirus`. Note that Twitter's Free/Basic API only allows searching with in the past 7 days. \n",
    "\n",
    "Somehow Twitter's official API guide didn't mention how to include Bearer token in the search. \n",
    "So here is a side info\n",
    "https://stackoverflow.com/questions/53002662/get-user-information-in-twitter-api-using-bearer-token\n",
    "\n",
    "See more at: \n",
    "* https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n",
    "* https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators\n",
    "\n",
    "##  Step 2.1: Get raw search result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Searching URL... https://api.twitter.com/1.1/search/tweets.json?q=%23coronavirus&lang=en&result_type=popular&count=5&tweet_mode=extended\nDone\n"
    }
   ],
   "source": [
    "def tag_search(bearer_token, search_url_base, query_tag, lang, result_type, count): \n",
    "    \"\"\"Basic search using tags \n",
    "\n",
    "    return all recent tweets of a tag, \n",
    "              in a specific language (e.g., 'en'), \n",
    "              in a result_type (mixed -- default, popular, recent), \n",
    "              for count (1 to 100, default 15) amount\n",
    "         as a list of dicts\n",
    "    \"\"\"\n",
    "    search_url = search_url_base + \"?q=\" +\\\n",
    "                 query_tag.replace(\"#\", \"%23\") + \"&\" +\\\n",
    "                 \"lang={}\".format(lang) + \"&\" +\\\n",
    "                 \"result_type={}\".format(result_type) + \"&\" +\\\n",
    "                 \"count={}\".format(count) + \"&\" + \\\n",
    "                 \"tweet_mode=extended\"\n",
    "\n",
    "    print (\"Searching URL...\", search_url)\n",
    "\n",
    "    request_headers = {\"Authorization\":\"Bearer \" + bearer_token}\n",
    "\n",
    "    request = urllib.request.Request(search_url, headers=request_headers)\n",
    "    reply = urllib.request.urlopen(request) \n",
    "    tweets = reply.read()\n",
    "    tweets = json.loads(tweets.decode('utf-8'))\n",
    "    \n",
    "    print (\"Done\")\n",
    "    return tweets['statuses']\n",
    "\n",
    "\n",
    "# To try it out, uncomment the lines below. \n",
    "bearer_token = credentials.bearer_token\n",
    "query_tag = \"#coronavirus\"\n",
    "search_url_base = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
    "tweets = tag_search(bearer_token, search_url_base, query_tag, \"en\", \"popular\", \"5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Distill the search result\n",
    "\n",
    "Twitter returns a very verbose information of the tweets. So you can distill down a little bit with certain information you care about. In the example below, we only keep information fields that are specified in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_tweets(tweets, info_keys, show_url):\n",
    "    \"\"\"\n",
    "    tweets: a list of tweets as dicts, result of extended search. \n",
    "\n",
    "    \"\"\"\n",
    "    counter  = 1 \n",
    "    new_tweets = []\n",
    "    for old_tweet in tweets: \n",
    "        new_tweet = {}\n",
    "        for key in info_keys:\n",
    "            if key == \"user_screen_name\":\n",
    "                new_tweet[key] = old_tweet[\"user\"][\"screen_name\"]\n",
    "            elif key == \"user_location\":\n",
    "                new_tweet[key] = old_tweet[\"user\"][\"location\"]\n",
    "            elif key == \"hashtags\":\n",
    "                new_tweet[key] = [x[\"text\"] for x in old_tweet[\"entities\"][\"hashtags\"]]\n",
    "            elif key == \"mentions\":\n",
    "                new_tweet[key] = [x[\"screen_name\"] for x in old_tweet[\"entities\"][\"user_mentions\"]]\n",
    "            else: \n",
    "                new_tweet[key] = old_tweet[key]\n",
    "        new_tweets.append(new_tweet)\n",
    "        if show_url:\n",
    "            print (str(counter)+  \".\", end = \" \")\n",
    "            print (\"By\", new_tweet['user_screen_name'], \"at\", new_tweet[\"created_at\"])\n",
    "            print(\"https://twitter.com/i/web/status/\"+old_tweet['id_str'])\n",
    "            print (new_tweet['full_text'])\n",
    "            print ()\n",
    "        counter  += 1 \n",
    "    return new_tweets\n",
    "\n",
    "# To try it out, uncomment lines below\n",
    "# info_keys = [\"full_text\", \"created_at\", ['user','screen_name']]\n",
    "# info_keys = [\"full_text\", \"created_at\", \"user_screen_name\", \"user_location\", \"hashtags\", \"mentions\"]\n",
    "# tweets[0].keys()\n",
    "# new_tweets = distill_tweets(tweets, info_keys, True)\n",
    "# new_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3: Save query results into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(dump_to, tweets, info_keys): \n",
    "    \"\"\"Dump distilled search results into a CSV file. \n",
    "\n",
    "    dump_to: str, path to the CSV to be written \n",
    "    tweets: list of dicts, keys are those in info_keys (see below), values are their respective data types\n",
    "    info_keys: list of a mixture of (strs or list of strs), \n",
    "               e.g., [\"full_text\", \"created_at\", ['user','screen_name']] , \n",
    "               list-type elements are concatenated with underscores, e.g., 'user_screen_name'\n",
    "    \"\"\"\n",
    "    if len(tweets) > 0 : \n",
    "        first_tweet = tweets[0]\n",
    "        keys = [x if type(x)==str else \"_\".join(x) for x in first_tweet]\n",
    "    else: \n",
    "        keys = []\n",
    "\n",
    "    with open(dump_to, 'w') as f:\n",
    "        header = \"\\t\".join(keys)\n",
    "        f.write(header + \"\\n\")\n",
    "        for tweet in tweets: \n",
    "            line = []\n",
    "            for key in keys: \n",
    "                if type(tweet[key]) == list:\n",
    "                    try: \n",
    "                        line.append( \",\".join(tweet[key])  ) \n",
    "                    except TypeError:\n",
    "                        print (key, tweet[key])\n",
    "                else: \n",
    "                    line.append(tweet[key])\n",
    "            line = \"\\t\".join(line)\n",
    "            line = line.replace(\"\\n\",\" \")#.replace(\"\\t\", \" \")\n",
    "            f.write(line + \"\\n\")\n",
    " \n",
    "    return None \n",
    "\n",
    "# save_csv(\"coronavirus.tsv\", new_tweets, info_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Put everything together \n",
    "\n",
    "If you just want something that works with everything in default, edit the last line. Specify a hashtag and how many results you want in return. \n",
    "\n",
    "With free/basic Twitter API, you can search up to 450 times in a 15-minute window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Searching URL... https://api.twitter.com/1.1/search/tweets.json?q=%23coronavirus&lang=en&result_type=popular&count=100&tweet_mode=extended\nDone\n\n Search result:\n"
    }
   ],
   "source": [
    "def lazy_guy_package(query_tag, how_many, info_keys):\n",
    "    bearer_token = credentials.bearer_token\n",
    "    search_url_base = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
    "    \n",
    "    tweets = tag_search(bearer_token, search_url_base, query_tag, \"en\", \"popular\", how_many)\n",
    "    \n",
    "    print (\"\\n Search result:\")\n",
    "    \n",
    "    new_tweets = distill_tweets(tweets, info_keys, False)\n",
    "\n",
    "    x= save_csv(query_tag[1:]+\"_search.csv\", new_tweets, info_keys)\n",
    "\n",
    "    return None \n",
    "\n",
    "# for hashtag in [\"#noplant19\", \"#plant19\", \"#harvest19\", \"#noharvest19\"]:\n",
    "for hashtag in [\"#coronavirus\"]:\n",
    "    lazy_guy_package(hashtag, 100, info_keys = [\"full_text\", \"created_at\", \"user_screen_name\", \"user_location\", \"hashtags\", \"mentions\"]) \n",
    "# just specify one hashtag, how many (15) most popular results in the past 7-days you want, and what are the fields of the tweets you care\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitb1d790ac39db43d4b1561ff0bd57be33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}